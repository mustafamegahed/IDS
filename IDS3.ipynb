{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jupyter Notebooks supports over 40 programming languages\n",
    "\n",
    "Packages\n",
    "    sys\n",
    "    time\n",
    "    matplotlib, matpotlib.pyplot\n",
    "    shiny\n",
    "    urllib, urllib.request\n",
    "    pandas\n",
    "    numpy\n",
    "    sciPy\n",
    "    seaborn\n",
    "    plotly\n",
    "    json\n",
    "    requests\n",
    "    os\n",
    "    PIL\n",
    "    IPython.display\n",
    "    nba_api\n",
    "    ibm_watson    # SpeechToTextV1    # LanguageTranslatorV3\n",
    "    random\n",
    "    re\n",
    "    graphviz\n",
    "    itertools\n",
    "    ibm_db\n",
    "    scikit-learn\n",
    "    statsmodels\n",
    "    %matplotlib inline    # Magic 7aga keda\n",
    "    %matplotlib notebook     # Active figures\n",
    "    %%html                # To run html code in a cell\n",
    "    %%capture             # ???\n",
    "    !wget\n",
    "    !pip\n",
    "    !python -V\n",
    "    !conda install python-graphviz --yes\n",
    "    \n",
    "    from ipywidgets import interact, interactive, fixed, interact_manual    # ???\n",
    "\n",
    "General Python Functions\n",
    "    def fun():\n",
    "        return\n",
    "\n",
    "    class smth(object):\n",
    "        def __init__(self):\n",
    "            \n",
    "    open\n",
    "    with open (file address, mode) as file:    # Closes the file after completion\n",
    "        \n",
    "    dir        \n",
    "    global\n",
    "    import\n",
    "    print    # print(\"\\nString : {}\".format(variable.tell()))\n",
    "    input\n",
    "    help\n",
    "    type\n",
    "    int\n",
    "    float\n",
    "    str\n",
    "    bool\n",
    "    len\n",
    "    sorted\n",
    "    del\n",
    "    range\n",
    "    enumerate\n",
    "    sum\n",
    "    max\n",
    "    min\n",
    "    zip\n",
    "    \n",
    "Expressions\n",
    "    /     # Division\n",
    "    //    # Integer division\n",
    "    +     # Addition  (Concatenating)\n",
    "    -     # Subtraction\n",
    "    *     # Multiplication\n",
    "    %     # Remainder\n",
    "    ()    # Parentheses\n",
    "\n",
    "Indexing & Slicing\n",
    "    variable[3]\n",
    "    variable[-1]\n",
    "    variable[-7]\n",
    "    variable[8:12]\n",
    "    variable[:4]\n",
    "    variable[6:]\n",
    "    variable[::2]\n",
    "    variable[0:5:2]\n",
    "    [:] (clone by value)\n",
    "\n",
    "Escape Sequences\n",
    "    \\n    # New line\n",
    "    \\t    # Tab\n",
    "    \\\\ or r\"string \\ string\"    # Back slash\n",
    "\n",
    "String Operations\n",
    "    .upper\n",
    "    .lower\n",
    "    .replace\n",
    "    .find\n",
    "    .split\n",
    "    .count\n",
    "\n",
    "Data Structrues\n",
    "    str, '', \"\"    # Immutable\n",
    "        .strip\n",
    "        .find\n",
    "    tuple, ()      # Immutable\n",
    "    list, []       # Mutable\n",
    "        .extend\n",
    "        .append\n",
    "        .sort\n",
    "        .reverse\n",
    "    set, {}\n",
    "        .add\n",
    "        .remove\n",
    "        .intersection, &\n",
    "        .difference\n",
    "        .union\n",
    "        .issubset\n",
    "        .issuperset\n",
    "    dict, {}       # Dictionary\n",
    "        .keys\n",
    "        .values\n",
    "    file           # Created with open\n",
    "        .name\n",
    "        .mode\n",
    "        .read\n",
    "        .close\n",
    "        .readline\n",
    "        .readlines\n",
    "        .write\n",
    "        .seek\n",
    "\n",
    "reserved words \n",
    "    in    # Used to check if a member exists in a data structure\n",
    "    \n",
    "Comparison Operators\n",
    "    ==    # Equal\n",
    "    !=    # Not equal \n",
    "    >     # Greater than\n",
    "    <     # less than\n",
    "    >=    # Greater than or equal to\n",
    "    <=    # Less than or equal to\n",
    "    \n",
    "Conditions\n",
    "    if condition:\n",
    "    elif condition:\n",
    "    else:\n",
    "    if not condition:\n",
    "    break\n",
    "    try:      # Code to try to execute\n",
    "    except:   # code to execute if there is an exception    # Or you can use an error with except and using except many times with only one try\n",
    "        # Ex:\n",
    "            try:\n",
    "                # code to try to execute\n",
    "            except ZeroDivisionError:\n",
    "                # code to execute if there is a ZeroDivisionError\n",
    "            except NameError:\n",
    "                # code to execute if there is a NameError\n",
    "            except:\n",
    "                # code to execute if ther is any exception\n",
    "            else:\n",
    "                # code to execute if there is no exception\n",
    "            finally:\n",
    "                # code to execute at the end of the try except no matter what\n",
    "        \n",
    "Logical operators\n",
    "and, &\n",
    "or, |\n",
    "not, ~\n",
    "\n",
    "Loops\n",
    "    for    #ex: for i in range(5)    #ex: for element in a list    #ex: for i, s in enumerate(list)\n",
    "    while condition:\n",
    "\n",
    "(lambda x: x.sample(sample_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-1-e318e2416897>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-e318e2416897>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    .read_csv\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "Pandas, pd\n",
    "\n",
    "    .read_csv\n",
    "    .read_excel\n",
    "    .Series\n",
    "    .DataFrame       # Filtering: df[df.columnname Comparison Operators value] \n",
    "        .head\n",
    "        .tail\n",
    "        .unique\n",
    "        .to_csv\n",
    "        .mean        # You can use (axis) as a parameter\n",
    "        .shape\n",
    "        .fillna      # Can be used to fill np.nan cells with any speciefied value\n",
    "        ._get_numeric_data    # To only use numeric data (columns)\n",
    "        .index\n",
    "            .name    # Can be used to rename the index\n",
    "        .value_counts\n",
    "        .columns\n",
    "            .values\n",
    "                .tolist\n",
    "        .str\n",
    "        .replace\n",
    "        .isin\n",
    "        .sum\n",
    "        .to_string\n",
    "        .sort_values\n",
    "        .reset_index\n",
    "        .groupby\n",
    "        .apply\n",
    "        .loc\n",
    "        .iloc\n",
    "        .idxmax    # To obtain the index of the maximum value of a certain column like: df['column'].idxmax()\n",
    "        .at[index, 'column']    # To obtain the value from a certain column at a certain index \n",
    "    .read_pickle\n",
    "    .set_option\n",
    "    .read_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Numby, np\n",
    "\n",
    "    .array    # Mutable\n",
    "        .dtype\n",
    "        .size\n",
    "        .ndim\n",
    "        .shape\n",
    "        .max\n",
    "        .min\n",
    "        .mean\n",
    "        .std\n",
    "        .median\n",
    "        .T\n",
    "    .dot\n",
    "    .pi\n",
    "    .sin\n",
    "    .linspace\n",
    "    .NaN       # To represent null values ex: df.replace('?',np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sklearn\n",
    "    .metrics\n",
    "        .accuracy_score\n",
    "        .confusion_matrix\n",
    "        .tree\n",
    "            .export_graphviz\n",
    "            .DecisionTreeClassifier\n",
    "                .fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matplotlib\n",
    "ax = plt.gca()   # ???\n",
    "\n",
    "fig = plt.gcf()  # ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data Science Methodology\n",
    "\n",
    "    From problem to approach:\n",
    "        1. What is the problem that you are trying to solve?\n",
    "        2. How can you use data to answer the question?\n",
    "    Working with the data:\n",
    "        3. What data do you need to answer the question?\n",
    "        4. Where is the data coming from (identify all sources) and how will you get it?\n",
    "        5. Is the data that you collected representative of the problem to be solved?\n",
    "        6. What additional work is required to manipulate and work with the data?\n",
    "    Deriving the answer:\n",
    "        7. In what way can the data be visualized to get the answer that is required?\n",
    "        8. Does the model used really answer the initial question or does it need to be adjusted?\n",
    "        9. Can you put the model into practice?\n",
    "        10. Can you get constructive feedback into answering the question?\n",
    "        \n",
    "        \n",
    "                                   Bussiness             Analytic\n",
    "                                 understanding  ------>  approach  ------\n",
    "                                                                         |\n",
    "                                                                         |\n",
    "                                                                           --->     Data    \n",
    "            Feedback    <--- ----------------                            |    requirements <---\n",
    "                           |                |                            |                    |\n",
    "                           |                |                            |                    |\n",
    "                           |                |           ---------------> --->     Data     ----\n",
    "            Deployment  <---                |           |                |     Collection  <---\n",
    "                           |                |           |                |                    |\n",
    "                           |                |           |                |                    |\n",
    "                           |                |           |                --->     Data     ----\n",
    "        --  Evaluation  <---                |           |                |    understanding\n",
    "        |                  |                |           |                |\n",
    "        |                  |                |           |                |\n",
    "        |                  -------         <-  ------>  -- Data          |\n",
    "        ------------------------->  Modelling  <------  preparation  <----\n",
    "        \n",
    "        \n",
    "        1. From Problem to Approach\n",
    "        2. From Requirements to Collection\n",
    "        3. From Understanding to Preparation    # Data Preparation takes up to 70% to 90% of time of the project\n",
    "        4. From Modeling to Evaluation\n",
    "        5. From Deployment to Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML\n",
    "    Tags\n",
    "        <html>\n",
    "        <head>\n",
    "        <body>\n",
    "        <title>\n",
    "        <table>\n",
    "        <h3>\n",
    "        <b>\n",
    "        <p>\n",
    "        <a>\n",
    "        <img>\n",
    "        <tbody>\n",
    "    Trees\n",
    "    Tables\n",
    "    \n",
    "#EX:\n",
    "\n",
    "%%html    # To run html code in a cell\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "\n",
    "    <head>\n",
    "    \n",
    "        <title>Page Title</title>\n",
    "        \n",
    "    </head>\n",
    "\n",
    "    <body>\n",
    "    \n",
    "        <h3><b id='boldest'>Text</b></h3>\n",
    "        <p>Text</p>\n",
    "        <h3><b>Text</b></h3>\n",
    "        <p>Text</p>\n",
    "        \n",
    "        <table>\n",
    "            <tr>\n",
    "                <td>cell_11</td>\n",
    "                <td>cell_12</td>\n",
    "                <td>cell_13</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>cell_21</td>\n",
    "                <td>cell_22</td>\n",
    "                <td>cell_23</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>cell_31</td>\n",
    "                <td>cell_32</td>\n",
    "                <td>cell_33</td>\n",
    "            </tr>\n",
    "        </table>\n",
    "        \n",
    "    </body>\n",
    "\n",
    "</html>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Beautiful Soup\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "page = requests.get('page_url').text      # html page as text\n",
    "soup = BeautifulSoup(page, 'html5lib')    # Or html.parser instead of html5lib\n",
    "\n",
    "BeautifulSoup\n",
    "    .find_all(name='tag')    # Them meaning of 'tag' is the same as ↓    # Returns a list    #find_all('a',href=True)\n",
    "    .prettify                # To display the HTML in the nested structure\n",
    "    .tag                     # Not actually the word tag but the any html tag the is included in the html page ex: title, h3, body, table ... # you can use method-chaining ex: soup.h3.b.string\n",
    "    .find                    # find(\"tag\",class_='table class'), using the class attribute of html tags, ex: <table class='pizza'>\n",
    "        .parent\n",
    "        .next_sibling\n",
    "        .attrs               # tag_child['id'], You can access a tag’s attributes by treating the tag like a dictionary, or tag_child.get('id')\n",
    "        .string              # Returns bs4.element.NavigableString, like a Python string (Unicode) with some Beautiful Soup additional features\n",
    "        \n",
    "    EX: Iterating over a table:\n",
    "        soup = BeautifulSoup(page, 'html5lib')\n",
    "        table_rows = soup.find_all(name='tr')\n",
    "        for i, row in enumerate(table_rows):\n",
    "            print(i, row)\n",
    "            cells = row.find_all('td')\n",
    "            for j, cell in enumerate(cells):\n",
    "                print(j, cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html = '''<!DOCTYPE html>\n",
    "<html>\n",
    "\n",
    "    <head>\n",
    "    \n",
    "        <title>Page Title</title>\n",
    "        \n",
    "    </head>\n",
    "\n",
    "    <body>\n",
    "    \n",
    "        <h3><b id='boldest'>Text</b></h3>\n",
    "        <p>Text</p>\n",
    "        <h3><b>Text</b></h3>\n",
    "        <p>Text</p>\n",
    "        \n",
    "        <table>\n",
    "            <tr>\n",
    "                <td>cell_11</td>\n",
    "                <td>cell_12</td>\n",
    "                <td>cell_13</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>cell_21</td>\n",
    "                <td>cell_22</td>\n",
    "                <td>cell_23</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>cell_31</td>\n",
    "                <td>cell_32</td>\n",
    "                <td>cell_33</td>\n",
    "            </tr>\n",
    "        </table>\n",
    "        \n",
    "    </body>\n",
    "\n",
    "</html>'''\n",
    "\n",
    "soup = BeautifulSoup(html, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql - structured query languages\n",
    "\n",
    "# Don't include <> when using actual values\n",
    "\n",
    "Used for relational databases to query data (tabular form with relationships between tabels)\n",
    "RDBMS - Relational Database Management System --> a set of software toolsthat controls the data (access, organization and storage)\n",
    "Ex: MySQL, Oracle Database, IBM Db2\n",
    "    \n",
    "Basic SQL Commands: Create , Insert, Select, Update, Delete  # Data Manipulation Language (DML) statments    # used to read and modify data\n",
    "\n",
    "Entity name = table name,\n",
    "Entity attributes = columns\n",
    "Data = rows\n",
    "    \n",
    "SELECT    # Selects data (columns/rows) from a table\n",
    "    Ex: select * from <table_name>      # * means all columns    # Don't include <>\n",
    "    Ex: select <column1_name, column2_name> from <table_name>    # The order of the columns displayied matches the order in the select statment\n",
    "    Ex: select * from <table_name> where <predicate>             # Where clause Ex: select * from <table_name> where <column1_name> = 'cell_value'    # 7aga keda like filtering\n",
    "    \n",
    "    Comparison Operators:\n",
    "        =     # Equal\n",
    "        <>    # Not equal \n",
    "        >     # Greater than\n",
    "        <     # less than\n",
    "        >=    # Greater than or equal to\n",
    "        <=    # Less than or equal to\n",
    "        \n",
    "    Expression\n",
    "        COUNT       # Retrieves the number of rows matching the query criteria\n",
    "            Ex: select count(<column_name>) from <table_name> where <column_name> = <'cell_value'>      # The first and second <column_name> do not have to be the same\n",
    "        \n",
    "        DISTINCT    # Retrieves unique values from a result set\n",
    "            Ex: select distinct <column_name> from <table_name> where <column_name> = <'cell_value'>    # The first and second <column_name> do not have to be the same \n",
    "        \n",
    "        LIMIT       # Restircts the number of rows retrieved from the databese\n",
    "            Ex: select * from <table_name> limit 10\n",
    "                \n",
    "CREATE    # Creates the table\n",
    "    create table <table_name>\n",
    "    (\n",
    "        <column1_name> <datatype> <optional parameters>,    # Look up the different data types\n",
    "        <column2_name> <datatype>\n",
    "    )\n",
    "\n",
    "INSERT    # Adds data (rows) to a table\n",
    "    Ex: insert into <table_name>\n",
    "            <(column1_name, column2_name, ...)>    # The number of values should be equal to the number of column names so that each column has a value\n",
    "        values\n",
    "            <('value_A1', 'value_A2', ...)>\n",
    "            <('value_B1', 'value_B2', ...)>        # Multible rows can be inserted at once\n",
    "            \n",
    "UPDATE    # Updtes one or more rows at a time\n",
    "    Ex: update <table_name>\n",
    "            set <column1_name> = 'cell_value'\n",
    "                <column2_name> = 'cell_value'          # Multible cells can be updated at once\n",
    "                where <column3_name> = 'cell_value'    # Without the where clause, all the rows will be updated\n",
    "            \n",
    "DELETE    # Deletes one or more rows at a time\n",
    "    Ex: delete from <table_name> where <table_name> in ('cell1_value', 'cell2_value')    # Without the where clause, all the rows will be deleted\n",
    "        \n",
    "Datatypes: \n",
    "    char(length)       # A character string of a fixed length\n",
    "    varchar(length)    # A character string of a variable length\n",
    "    bigint             # A number up to 19 digits long\n",
    "    \n",
    "Optional Parameters:\n",
    "    PRIMARY KEY    # Preventing duplication of data (rows)\n",
    "    NOT NULL       # columns cannot contain a null value\n",
    "    \n",
    "The key advantage of the relational database is logical and physical data independence and storage independence\n",
    "Entities (tables) are independent objects which have attributes (columns)\n",
    "Primary Key: Primary key of a relational table uniquely identifies each tuple or row in a table preventing duplication of data and providing a way of defining relationships between table\n",
    "Foreign Key: Tables can contain foreign keys which are primary keys defined in other tables creating links between the tables\n",
    "Cloud databases (Oracle Database, Microsoft Azure SQL Datacbase, Amazon Database)\n",
    "\n",
    "Data Definition Language statment (DDL) vs Data Minipulation Language Statment (DML)\n",
    "    DDL => used to Define, Change or Drop Database objects such as tables, such as:\n",
    "        CREATE   -> is used for creating tables and defining its columns\n",
    "        ALTER    -> is used for altering tables including adding and dropping columns and modifying their data types\n",
    "            Ex: alter table <table_name>\n",
    "                    add column <column1_name> <datatype>\n",
    "                    add column <column2_name> <datatype>;\n",
    "\n",
    "            Ex: alter table <table_name>\n",
    "                    alter column <column_name> \n",
    "                        set data type <datatype>;\n",
    "                    \n",
    "            Ex: alter table <table_name>\n",
    "                    drop column <column_name>;\n",
    "                    \n",
    "        TRUNCATE -> is used for deleting data in a table but not the table itself\n",
    "            Ex: truncate table <table_name>\n",
    "                    immediate;                  # specifies to process the statment immediatly and that it cannot be undone \n",
    "                    \n",
    "        DROP     -> is used for deleting tables\n",
    "            Ex: drop table <table_name>;\n",
    "        \n",
    "    DML => used to read and modify data in tables (CRUD Operations => Create, Read, Update, Delete rows)\n",
    "        INSERT -> is used for inserting rows of data into a table\n",
    "        SELECT -> is used for selecting or reading rows of data from a table\n",
    "            String Pattern:\n",
    "                Ex: where <column_name> like <'letter%' or 'pattern%'>    # The % is used to represent messing letters, and can be placed before the pattern, after the pattern or both    # The % is called a Wildcard character (substitutes other characters)\n",
    "            Ranges:\n",
    "                EX: where <column_name> >= number and <column_name> <= number    or      where <column_name> between number and number\n",
    "            Sets of Values:\n",
    "                Ex: where <column_name> = <'cell_value'> or/and <column_name> = <'cell_value'>\n",
    "                Ex: where <column_name> in (<'cell_value'>, <'cell_value'>)  \n",
    "                    \n",
    "            Sorting the Result Set --> ORDER BY clause\n",
    "                Ex: select * from <table_name> order by <column_name>         # The default is ascending    # Instead of the column name, you can us ethe number of the column, 1, 2, 3 ...\n",
    "                Ex: select * from <table_name> order by <column_name> DESC    # To sort in descending order\n",
    "            \n",
    "            Gouping the Result Set --> GROUP BY clause\n",
    "                Ex: select distinct(<column_name>) from <table_name>    # Selecting unique values\n",
    "                Ex: select <column_name>, count(<column_name>) from <table_name> group by <column_name>    # For the column created by count you can use (as <column_name>) to name the newly calculated column\n",
    "                Ex: select <column_name> from <table_name> group by <column_name> having <column_name> > number    # The HAVING clause works only with the GROUP BY clause while the WHERE clause is for the intire result set\n",
    "            \n",
    "        UPDATA -> is used for editing rows in a table\n",
    "        DETELE -> is used for removing rows from a table\n",
    "        \n",
    "Built in Database functions\n",
    "    Aggregate or Column Functions ex: sum, min, max, avg    # Perform operations on columns and outputs a single value\n",
    "        Ex: select sum(<column_name>) from <table_name>    # You can use (as <name>) after sum to name the calculated value    # You can add the WHERE clause\n",
    "        Ex: select avg(<column1_name> / <column2_name>) from <table_name>    # Mathematical operations can be performed between columns \n",
    "    \n",
    "    Scaler and String Functions ex: round, length, ucase, lcase    # Perform operations on every input value    # Can be used in the WHERE clause like (ucase, lcase) to match values \n",
    "        Ex: select round(<column1_name>) from <table_name>\n",
    "        Ex: select round(avg(<column1_name>)) from <table_name>    # A function operating on the output of another function\n",
    "            \n",
    "    Date and Time Functions: \n",
    "        Datatypes:\n",
    "            date       YYYYMMDD\n",
    "            time       HHMMSS\n",
    "            timestamp  YYYYXXDDHHMMSSZZZZZZ\n",
    "\n",
    "        Functions: to extract portions on the time\n",
    "            year, month, day, dayofmonth, dayofweek, dayofyear, week, hour, minute, second    # Can be used in the WHERE clause \n",
    "            Ex: select day(<column_name>) from <table_name>\n",
    "        \n",
    "        Date and Time Arithmetic:\n",
    "            Ex: select (<column_name> + 3 days) from <table_name>\n",
    "        \n",
    "        Special Registers:\n",
    "            current_date, current_time\n",
    "            \n",
    "Sub-queries and Nested Selects\n",
    "    Ex: select <column1_name> from <table_name> where <column2_name> = (select max(<column2_name>) from <table_name>)    \n",
    "    Error:   select * from <table_name> where <column_name> > avg(<column2_name>)    # invalid use of the aggregate function\n",
    "    correct: select * from <table_name> where <column_name> > (select avg(<column2_name>) from <table_name)>    # In the WHERE clause\n",
    "        \n",
    "    Error  : select <column1_name>, <column2_name>, avg(<column3_name>) from <table_name>\n",
    "    correct: select <column1_name>, <column2_name>, (select avg(<column3_name>) from <table_name>) from <table_name>    # Column Expression    # List of columns\n",
    "        \n",
    "    Ex:  select * from (select <column1_name>, <column2_name>, avg(<column3_name>) from <table_name>) as <new_table_name>    # Table Expression    # Useful when working with multible tables or doing joins    # In the FROM clause\n",
    "\n",
    "Access Multible Tables in The Same Query\n",
    "    Sub-queries\n",
    "        Ex: select * from <table1_name> where <column_name> in (select <column_name> from <table2_name>)    # The WHERE clause can be used in the sub-query\n",
    "            \n",
    "    Implicit JION\n",
    "        Ex: select * from <table1_name>, <table2_name>    # Full jion \n",
    "        Ex: select * from <table1_name>, <table2_name> where <table1_name>.<column1_name> = <table2_name>.<column2_name>\n",
    "        Ex: select * from <table1_name> A, <table2_name> B where A.<column1_name> = B.<column2_name>    # A, B ar any other alias for the table name can be used instead of the table name \n",
    "        Ex: select A.<column1_name>, B.<column2_name> from <table1_name> A, <table2_name> B where A.<column_name> = B.<column_name>\n",
    "        \n",
    "    JION Operators (INNER, OUTER)\n",
    "    \n",
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Access Databases Using Python                     # DB-API is Pythons's standard API for accessing relational databases (can be used to access any database with Python)\n",
    "    SQL API\n",
    "        from dbmodule import connect\n",
    "        # Create a connection object\n",
    "        connection = connect(database name, username, password)\n",
    "        # Create a cursor object\n",
    "        cursor = connection.cursor()\n",
    "        # Run queries\n",
    "        cursor.execute('query')    # send('query')\n",
    "        # Results\n",
    "        results = cursor.fetchall()    # status_check()\n",
    "        # Free resources\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        \n",
    "        # Storing the data (this line is not in its correct order in the code)\n",
    "        import pandas as pd\n",
    "        df = pd.read_sql('query', connection)    # Query like: select * from <table_name>\n",
    "        # Then you can complete exploring the data\n",
    "        \n",
    "        # The two main concepts in Python DB-API\n",
    "            # Connection Objects\n",
    "                Database connections\n",
    "                Manage transactions\n",
    "\n",
    "            # Cursor Objects (query objects)\n",
    "                Database queries\n",
    "                Scroll through result set\n",
    "                Retreive results\n",
    "            \n",
    "        # Connection Methods\n",
    "            .cursor     # Returns a new cursor object using the connection\n",
    "            .commit     # Commit any pending transaction to the database\n",
    "            .rollback   # Causes the database to roll back to the start of any pending transaction\n",
    "            .close      # Close the database connection\n",
    "            \n",
    "        # Cursor Methods\n",
    "            # used to manage the content of a fetch operation\n",
    "            .callproc\n",
    "            .execute\n",
    "            .executemany\n",
    "            .fetchone\n",
    "            .fetchmany\n",
    "            .fetchall\n",
    "            .nextset\n",
    "            .arraysize\n",
    "            .close\n",
    "            # Cursors created from the same connection are not isolated (any changes that is done to the database by a cursor are immediatly visible by the other cursors)\n",
    "            # Cursors created from different connections can or cannot isolated depending on how the transaction support is implemented\n",
    "            # A cursor is a like a file name or a file handle\n",
    "        \n",
    "        # Database connection credetials: dsn_driver, dsn_database, dsn_hostname, dsn_port, dsn_protocol, dsn_uid, dsn_pwd\n",
    "        \n",
    "    SQL Magic commands can be used to execute queries more easily from Jupyter Notebooks\n",
    "        Magic commands have the general format %sql select * from tablename\n",
    "        Cell magics start with a double %% (percent) sign and apply to the entire cell\n",
    "        Line magics start with a single % (percent) sign and apply to a particular line in a cell\n",
    "    \n",
    "    # Tips\n",
    "        # To select data from a column with a mixed case name, you need to specify the column name in its correct case in double quotes \"Column\" (not single quotes)\n",
    "        # If the name of the column contains spaces, special characters, brackets..., by default the database may map them to underscores (put the name of the column in double quotes \" \")\n",
    "        # If the query contains double quotes like and you want to store it in a Python variable, use single outer double quotes like: query_statment = 'select \"Column\" from table'\n",
    "        # A backslash \\ can be used as an escape character in the query like \\'Column\\'    # A backslash \\ can also be used to split the query on multible lines (in Python notebooks)\n",
    "        # %%sql can be used to make the intire cell execute sql queries (interpreted by sql magic) (you don't need a backslash \\ to split lines)\n",
    "        # Use LIMIT to restrict the number of rows retrieved\n",
    "    \n",
    "    # Getting Table and Columns Details\n",
    "        # To get the list of tables in the database, use 'INFORMATION_SCHEMA.YABLES' for SQL Server (for other databases, check online) Ex: select * from INFORMATION_SCHEMA.YABLES.\n",
    "        # You can get system tables using the above query like creation time and many more table (check online)\n",
    "        # You can get all the information about the tables and their columns\n",
    "        \n",
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Advanced SQL\n",
    "        # A View\n",
    "            # Representing data of one or more tables, a view can include all or some of the columns from one or more base tables or existing views\n",
    "            # A view creates a named specification of results table and can be queried in the same way as a table, and you can update the base tables by updating the view\n",
    "            # The definition of the view is stored and the data is stored in the base tables not the view itself    # Views are dynamic (they change as the date in the tables change)\n",
    "            # A view can be used to:\n",
    "                # Show a selection of tables for a given table so you can omit sensitive information\n",
    "                # Combine two ore more tables\n",
    "                # Symplify access to data by granting access to a view without granting access to the base tables\n",
    "                # Show only the portions of data relevant to the process that uses the view\n",
    "                Ex: create view <view_name> (<column1_alias>, <column2_alias>, <column3_alias>,...)    # The alias part is optional\n",
    "                        as select <column1_name>, <column2_name>, <column3_name>,...\n",
    "                        from <table_name> where <predicate>           # The WHERE clause is optional... group by, having also work but cannot use order by clause or name a host variable\n",
    "                # A view can be used in select statements or the creation of other views acting like a table (can be queried like a table)\n",
    "                Ex: drop view <view_name>    # To remove a view\n",
    "        \n",
    "        # Stored Precedure\n",
    "            # A set of SQL statements that are stored and executed on the database server by the client by sending only one statement\n",
    "            # They can perform create, read, update and delete operations and return results to the uplication\n",
    "            # Benefits:\n",
    "                # Reduction in the network traffic\n",
    "                # Improvement in performance (executes on the cloud and only the result is sent back)\n",
    "                # Reuse of code    # Increased security\n",
    "                \n",
    "                    Ex: create procedure <procedure_name> (<parameter1>, <parameter>, ...) language <language like SQL>\n",
    "                    begin\n",
    "                    <\n",
    "                    procedural logic\n",
    "                    >\n",
    "                    end\n",
    "                    \n",
    "            # Procedures can be called from External applications or Dynamic SQL statemnts\n",
    "            Ex: call <procedure_name> (parameters)\n",
    "                \n",
    "        # ACID Transactions\n",
    "            # Indivisible unit of work, It consists of one or more sql statements\n",
    "            # To be successful either all happens or none (all the statements complete leaving the database in a new stable state, or leaving the data as it was before the transaction begins if one or more statements were not successful to keep the data in a consistent state)\n",
    "            # ACID:\n",
    "                # A -> Atomic     -> All changes must be performed successfully or not at all\n",
    "                # C -> Consistent -> Data must be in a consistent state before and after the transaction\n",
    "                # I -> Isolated   -> No other process can change the data while the transaction is running\n",
    "                # D -> Durable    -> The changes made by the transaction must presist\n",
    "                \n",
    "            Ex: begin\n",
    "                    <statment1>\n",
    "                    \n",
    "                    <statment2>\n",
    "                    \n",
    "                    <statment3>\n",
    "                commit      # Successful\n",
    "                rollback    # Failed\n",
    "                \n",
    "            # Use EXEC SQL command to execute SQL statements from code (C, R, Python, ...)\n",
    "            \n",
    "        # Join\n",
    "            # Combines rows from two or more tables based on a relationship using Primary keys and Foreign keys (To combine data from three or more different tables, you can simply add new joins to the SQL statement)\n",
    "            # You can use a self-jion to compare rows within the same table\n",
    "            # Inner join\n",
    "                # Displays only the rows from two tables that having matching value in a commom column usually the primary key of one table that exists as a foreign key in the second table\n",
    "                Ex: select A.<column1>, A.<column2>, B.<column1>, B.<column2>\n",
    "                        from <table1> A inner join <table2> B on A.<column1> = B.<column1>\n",
    "                        \n",
    "            # Outer join\n",
    "                # Left outer\n",
    "                # All rows from the left table and any matching rows from the right table\n",
    "                    Ex: select A.<column1>, A.<column2>, B.<column1>, B.<column2>\n",
    "                        from <table1> A left join <table2> B on A.<column1> = B.<column1>\n",
    "                        \n",
    "                # Right outer\n",
    "                    # All rows from the right table and any matching rows from the left table\n",
    "                    Ex: select A.<column1>, A.<column2>, B.<column1>, B.<column2>\n",
    "                        from <table1> A right join <table2> B on A.<column1> = B.<column1>\n",
    "                \n",
    "                # Full outer\n",
    "                    # All rows from both tables\n",
    "                    Ex: select A.<column1>, A.<column2>, B.<column1>, B.<column2>\n",
    "                        from <table1> A full join <table2> B on A.<column1> = B.<column1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data Analysis with Python (EDA & Getting data ready for ML)\n",
    "    # Importing Datasets\n",
    "\n",
    "    # Understanding the problem\n",
    "    \n",
    "    # Understading the Data\n",
    "    \n",
    "    # Python Packages for Data Science\n",
    "    \n",
    "        # Scientific Computing Libraries\n",
    "            # Pandas\n",
    "            # NumPy\n",
    "            # SciPy\n",
    "        \n",
    "        # Visualization Libraries\n",
    "            # Matplotlib\n",
    "            # Seaborn\n",
    "        \n",
    "        # Algorithmic Libraries\n",
    "            # Scikit-learn\n",
    "            # Statsmodels\n",
    "    \n",
    "    # Importing and Exporting Data in Python\n",
    "        # pd.read_csv()       # pd.to_csv()\n",
    "        # pd.read_json()      # pd.to_json()\n",
    "        # pd.read_excel()     # pd.to_excel()\n",
    "        # pd.read_sql()       # pd.to_sql()\n",
    "        # pd.read_hdf()       # df.to_hdf()\n",
    "    \n",
    "    # Getting Started Analyzing Data\n",
    "        df.head()                                             # Also df['column'].head()\n",
    "        df.tail()\n",
    "        df.columns()                                          # Could be used to change columns names by  assigning it to a list with columns names\n",
    "        df.replace('?',np.NaN)\n",
    "        df.dropna(subset=[\"column\"], axis=0, inplace=True)    # To drop missing values along a column \n",
    "        df.dtyps()\n",
    "        df.describe()                                         # By default skips row as and columns that do not contain numbers, can be included ny using (include='all')\n",
    "        df.info()\n",
    "    \n",
    "    # Accessing Databases with Python\n",
    "        from dbmodule import connect\n",
    "        connection = connect()\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute()\n",
    "        results = cursor.fetchall()\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "    \n",
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # Data Cleaning, Data Wrangling \n",
    "\n",
    "        # Pre-processing Data in Python \n",
    "\n",
    "        # Missing Values                                          # '?', 'N/A', '0', 'NaN',...\n",
    "            df.dropna(subset=['column'], axis, inplace=True)      # Drop the variable (axis=1 --> drop column) or drop the data entry (axis=0 --> drop row)    # (inplace=True) to allow the modification to be done on the dataset directly    # (subset=['column']) to select the wanted column\n",
    "            df.replace(missing_value, new_value)                  # Replace with an average (numerical), by frequency or based on other function or leave it as missing\n",
    "            df.isnull()                                           # The output is a boolean value indicating whether the value that is passed into the argument is in fact missing data\n",
    "            df.notnull()\n",
    "            df.fillna('value')                                    # Fill missing values with 0\n",
    "            df.column.value_counts()\n",
    "            --------.idxmax()\n",
    "            df.reset_index()\n",
    "            np.nan == NaN                                         # Use np.nan to express missing values\n",
    "\n",
    "        # Data Formatting\n",
    "            df.rename(columns{'old_name':'new_name'}, inplace=True)\n",
    "            df.dtypes()                                           # To identify data types\n",
    "            df.astype()                                           # To convert data types    # df.column = df.column.astype('int64')    # See (copy=True) parameter \n",
    "            df['column3'] = df['column1'] * df['column2']\n",
    "\n",
    "        # Data Normalization (Centering, Scaling)                 # To prevent large variables from influncing the model in an unreasonable way\n",
    "\n",
    "            # Simpele Feature Scaling                             # New values range between 0 and 1\n",
    "                Xnew = Xold / Xmax    \n",
    "\n",
    "            # Min-Max                                             # New values range between 0 and 1\n",
    "                Xnew = (Xold - Xmin) / (Xmax - Xmin)\n",
    "\n",
    "            # Z-score                                             # New values hover around 0, typaclly ranging between -3 and 3 but can be higher or lower \n",
    "                Xnew = Xold - μ / σ                               # μ --> average, σ --> standard deviation\n",
    "\n",
    "        # Binning in Python                                       # Converts numerical variables into catagorical variables\n",
    "            np.linspace(min(df['column']), max(df['column']), number of dividers_n)             # Returns an array of n equally spaced numbers over the specified interval of the numerical column    # Store it in bins\n",
    "            pd.cut(df['column'], bins, labels=list of intervals labels, include_lowest=True)    # Store it in a new column df['new_column']    # Use histograms\n",
    "\n",
    "        # Turning Categorical Variables into Quantitative Variables (Dummy Variables) (one-hot encoding)\n",
    "            pd.get_dummies(df['column']) \n",
    "            pd.concat([df_1, df_2], axis=1)            # Merges \"df_1\" and \"df_2\"\n",
    "            df.drop('column', axis=1, inplace=True)    # To drop a column from a dataframe\n",
    "            \n",
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # Exploratory Data Analysis (EDA)\n",
    "\n",
    "        # Exploratory Data Analysis\n",
    "            # Sumarize main characteristics of the data\n",
    "            # Gain main understading of the data\n",
    "            # Uncover relationships between variables\n",
    "            # Extract important variables\n",
    "            \n",
    "        # Descriptive Statistics\n",
    "            df.decsribe()                             # By default skips row as and columns that do not contain numbers, can be included by using (include='all') or instead of all choose a speciefic variable type    # Used mainly with numerical variables\n",
    "            df['column'].value_counts().to_frame()    # Used mainly with categorical variables\n",
    "            df['column'].unique()\n",
    "            sns.boxplot(x, y, data)                   # x --> predictor variable (independent)    # y --> target variable (dependent)\n",
    "            sns.scatterplot(x, y, data)               # Used with continuous variables\n",
    "            plt.scatter(x = df['column1'], y = df['column2'])\n",
    "            plt.title('title')\n",
    "            plt.xlabel('xlabel')\n",
    "            plt.ylabel('ylabel')\n",
    "\n",
    "        # GroupBy in Python\n",
    "            df.groupby(['column1', 'column2'], as_index=False).mean()    # .mean() or .max() or any aggregate function\n",
    "                # Used on categorical variables to group the data into subsets based on that categorical variable\n",
    "                # Can group be a single or multiple variables\n",
    "            df.pivot(index='column1', columns='column2')                 # Usually the output (df) from groupby()\n",
    "            plt.pcolor(df, cmap='RdBu')                                  # Usually the output (df) from pivot()    # cmap --> changes the map colors\n",
    "            plt.colorbar()                                               # To display a color bar beside the heatmap\n",
    "\n",
    "        # Correlation (Correlation doesn't imply causation)\n",
    "            df.corr()    # Correlation between continuous variables    # Pearson Correlation Coefficient is the default method of the function \"corr\" (the same as the one calculated using 'stats.pearsonr' but 'stats.pearsonr' has the p-value calculated as well)\n",
    "            df[['column1', 'column2']].corr()    # Correlation between two continuous variables\n",
    "            sns.regplot(x, y, data)    \n",
    "                # The variables that show a strong postive or negative relationship with the target variable can be used to predict the target variable\n",
    "                # The variables that show a weak relationship with the target variable cannot be used to predict the target variable\n",
    "            plt.ylim(0,)    # To make the y axis stars at 0\n",
    "            \n",
    "        # Correlation - Statistics (Pearson correlation measures the correlation between two continuous variables)\n",
    "            # Correlation Coefficient\n",
    "                # Close to +1 --> Large Postive Relationship\n",
    "                # Close to -1 --> Large Negative Relationship\n",
    "                # Close to 0 --> No Relationship\n",
    "\n",
    "            # P-value    # The P-value tells us how certain we are about the correlation we calculated\n",
    "                # The P-value is the probability value that the correlation between these two variables is statistically significant. Normally, we choose a significance level of 0.05, which means that we are 95% confident that the correlation between the variables is significant\n",
    "                    # P-value < 0.001 --> Strong certainty in the result\n",
    "                    # P-value < 0.05 --> Moderate certainty in the result\n",
    "                    # P-value < 0.1 --> Weak certainty in the result\n",
    "                    # P-value > 0.1 --> No certainty in the result\n",
    "\n",
    "            # Strong Correlation \n",
    "                # Correlation coefficient close to 1 or -1\n",
    "                # P-value less than 0.001\n",
    "            \n",
    "            from scipy import stats\n",
    "            stats.pearsonr(df['column1'], df['column2'])    # pearson_coef, p_value = stats.pearsonr()    # You can do this for all the variables with the target variable\n",
    "            # Correlation Heatmap    # Shows how all of the variables are correlated to one another and most importantly to the target variable\n",
    "\n",
    "        # Association between two categorical variables: Chi-Square\n",
    "            # The Chi-square tests a null hypothesis that the variables are independent\n",
    "            # The test compares the observed data to the values that the model expects, if the data was distributed in different categories by chance\n",
    "            # Anytime the observed data doesn't fit within the model of the expected values, The probability that the variables are dependent becomes stronger, thus proving the null hypothesis incorrect\n",
    "            # The chi-square test doesn't tell you the type of the relationship between both variables but only that a relationship exists\n",
    "        \n",
    "            X^2 = Σ (Oi - Ei)^2 / Ei    # The formula of chi-square\n",
    "                # Oi --> observed value\n",
    "                # Ei --> expected value\n",
    "                # X^2 --> chi-square \n",
    "                \n",
    "            Ei = Row total * Column total / Grand total\n",
    "            Degree of freedeom = (row -1) * (column -1)    # See the chi-square table\n",
    "            scipy.stats.chi2_contingency()    \n",
    "                # the function will print out the chi-square test value\n",
    "                # The second value is the p-value\n",
    "                # The third value is the degree of freedom\n",
    "                # the forth is an array of the expected values\n",
    "                \n",
    "            # P-value of < 0.05, we reject the null hypothesis that the two variables are independent and conclude that there is evidence of association between the two variables\n",
    "            # ANOVA: Analysis of Variance\n",
    "                # A statistical method used to test whether there are significant differences between the means of two or more groups.\n",
    "                # F-test score\n",
    "                    # ANOVA assumes the means of all groups are the same, calculates how much the actual means deviate from the assumption, and reports it as the F-test score. \n",
    "                    # A larger score means there is a larger difference between the means\n",
    "                    # P-value tells how statistically significant our calculated score value is\n",
    "                    # Since ANOVA analyzes the difference between different groups of the same variable, the groupby function will come in handy. \n",
    "                    # Because the ANOVA algorithm averages the data automatically, we do not need to take the average before hand.\n",
    "                    \n",
    "                grouped_df = df[['variable', 'target']].groupby(['variable'])\n",
    "                # We can use the function 'f_oneway' in the module 'stats' to obtain the F-test score and P-value.\n",
    "                f_val, p_val = stats.f_oneway(grouped_df.get_group('category1')['target'], grouped_df.get_group('category2')['target'], grouped_df.get_group('category3')['target'])\n",
    "                    # You can use all categories of the categorical variable that is used in the groupby function at once or use each two categories together\n",
    "                    \n",
    "            # See crosstab function in pandas\n",
    "            \n",
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        # At the end of this stage, you should be able to know which variables are important so you can use them in developing the model\n",
    "        # You should neglect the unimportant variables so your model is simpler and more effecient\n",
    "        # You can feature-engineer new variables from all the variables (important & unimportant variables) as long as the new variable has a strong correlation with the target variable\n",
    "        # list the final variables:\n",
    "            # Continuous numerical variables:\n",
    "                # ...\n",
    "                # ...\n",
    "            # Categorical variables:\n",
    "                # ...\n",
    "                # ...\n",
    "                \n",
    "        # Final note: The variables that will be used in developing the model should be:\n",
    "            # Highly correlated with the target variable\n",
    "            # Weakly correlated with each other    <-- di men 3andy bas ta2reeban s7 so look it up online**\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data Analysis with Python (ML)\n",
    "    # Model Development\n",
    "\n",
    "        # Model Development\n",
    "            # A model can be thought of as a mathematical equation used to predict a  value given one or more other values\n",
    "            # Or relating the independent variables to the dependent variable\n",
    "\n",
    "        # Simple linear Regression - SLR (one independent variable) and Multiple Linear Regression - MLR (multiple independent variables)    # The target variable must be continuous\n",
    "            # SLR\n",
    "                y = b0 + b1*x               # To fit the model is to find the parameters --> b0 & b1 values that makes the most accurate model\n",
    "                    # y  --> the dependent variable (target)      # y(hat) --> the predicted values\n",
    "                    # x  --> the independent variable (feature or predictor)\n",
    "                    # b0 --> the intercept\n",
    "                    # b1 --> the slope\n",
    "                         \n",
    "                from sklearn.linear_model import LinearRegression\n",
    "                lm = LinearRegression()\n",
    "                X = df['predictor']\n",
    "                Y = df['target']\n",
    "                lm.fit(X, Y)\n",
    "                y_hat = lm.predict(X)\n",
    "                lm.intercept_    # b0\n",
    "                lm.coef_         # b1 (slope)\n",
    "                \n",
    "            # MLR\n",
    "                y = b0 + b1*x1 + b2*x2 + ... + bn*xn              # If there are only two predictors the values can be visualized on a 3D space\n",
    "                    # y  --> the dependent variable (target)      # y(hat) --> the predicted values\n",
    "                    # x1  --> the independent variable_1 (feature_1 or predictor_1)\n",
    "                    # x2  --> the independent variable_2 (feature_2 or predictor_2)\n",
    "                    # xn  --> the independent variable_n (feature_n or predictor_n)\n",
    "                    \n",
    "                    # b0 --> the intercept\n",
    "                    # b1 --> the coefficient or parameter of x1\n",
    "                    # b2 --> the coefficient or parameter of x2\n",
    "                    # bn --> the coefficient or parameter of xn\n",
    "                \n",
    "                from sklearn.linear_model import LinearRegression\n",
    "                lm = LinearRegression()\n",
    "                X = df[['predictor_1', 'predictor_2', 'predictor_n']]    # You can use 'Z' to represent them instead of 'X'\n",
    "                Y = df['target']\n",
    "                lm.fit(X, Y)\n",
    "                y_hat = lm.predict(X)\n",
    "                lm.intercept_    # b0\n",
    "                lm.coef_         # b1, b2, ..., bn \n",
    "                \n",
    "        # Model Evaluation using Visualization\n",
    "            import seaborn as sns\n",
    "            sns.regplot(x='predictor', y='target', df)\n",
    "            plt.figure(figsize=(width, height))\n",
    "            plt.ylim(0,)\n",
    "                # Shows the relationships between two variables\n",
    "                # The strength of the correlation\n",
    "                # The direction of the relation (positive or negative)\n",
    "            sns.residplot(x='predictor', y='target', df)\n",
    "                # Represents the error between the predicted value 'y(hat)' and the actual value 'y' --> y(hat) - y\n",
    "                # Then we plot that value on the vertical axis with the independent variable as the horizantal axis\n",
    "                # And repeat that for each data point\n",
    "                # We expect the result to have a zero mean and distributerd evenly arount the x axis with similar variance\n",
    "                # And there is no curvature\n",
    "                # --> This type of residual plot suggests a linear model is appropriate\n",
    "                \n",
    "                # If there is a curvature (the values of the error changes with x) --> That suggests a nonlinear model\n",
    "                \n",
    "            # Distribution plots\n",
    "                ax1 = sns.displot(df['target'], hist=False, color='r', label='Actual Values')    # 'hist' is set to false because of the target \n",
    "                sns.displot(y_hat, hist=False, color='b', label='Fitted Valuea', ax=ax1)\n",
    "                plt.title('title')\n",
    "                plt.xlabel('xlabel')\n",
    "                plt.ylabel('ylabel')\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "                plt.close()    # ???\n",
    "                \n",
    "                # Distribution plots count the predicted values versus the actual values\n",
    "                # Useful for visualizing model with more than one independent variable\n",
    "                # It plots the distribution of the actual values versus the predicted values\n",
    "                \n",
    "            # MLR - Distribution plots\n",
    "\n",
    "        # Polynomial Regression and Pipelines    --> If a linear model is not a good fit for the data\n",
    "            # We transfrom the data into a polymonial then use linear regression to fit the parameter\n",
    "            # Polynomial Regression is a special case of the general linear regression\n",
    "            # Useful for describing curvilinear relationships\n",
    "            # Curvilinear relationship is what you get by squaring or setting higher order terms of the predictor variables\n",
    "            \n",
    "                # Quadratic - 2nd order polynomial regression - one dimentional\n",
    "                    y_hat = b0 + b1*x1 + b2*(x1)**2                                   # The predictor variable is squared\n",
    "                    \n",
    "                # Cubic - 3rd order polynomial regression - one dimentional\n",
    "                    y_hat = b0 + b1*x1 + b2*(x1)**2 + b3*(x1)**3                      # The predictor variable is cubed\n",
    "                     \n",
    "                # Higher order polynomial regression - one dimentional                # Careful not to overfit! \n",
    "                    y_hat = b0 + b1*x1 + b2*(x1)**2 + b3*(x1)**3 + ...\n",
    "                \n",
    "                # In all cases, the relationship between the variable and the parameter is always linear <-- Mesh fahem ezzay, t2reeban y2sod bel parameters b0, b1, ... bas bardo ezzay ???\n",
    "                    \n",
    "                    # This happens when using PolynomialFeatures from sklearn.preprocessing bas when using 'np.polyfit' and 'np.poly1d' I have no idea about what happens\n",
    "                        # Hwa be3mel new data frame feha new features Ex: \n",
    "                            # lw 3andy one feature fel original data frame (x1) w 3ayz a3mal 3rd order polynomial regression \n",
    "                            # El new data frame hykoon feha three features x1, (x1)**2, (x1)**3\n",
    "                            # B3d keda we will use linear regression to find the parameters b0, b1, b2 as if el features di kanet totally independent from the beginning  <-- ta2reeban di 3'alat\n",
    "                            # I'm not sure about the last one because if so, How can we still plot the model in 2D (quadratic, cubic, ... all can be ploted in 2D)\n",
    "                            # **T2reeban lw ana fahem s7: hwa by3ml keda bas to find the parameters b0, b1, b2.. b3dan we use the equation in one dimention 3ady\n",
    "                \n",
    "                   \n",
    "\n",
    "                f = np.polyfit(x, y, degree)                      # Fit the polynomial using the function 'polyfit'\n",
    "                p = np.poly1d(f)    # p --> the model equation    # Use the function 'poly1d' to display the polynomial function.\n",
    "                \n",
    "                \n",
    "                # Polynomial regression with more than one dimention\n",
    "                    # 2nd order polynomial regression - two dimentional\n",
    "                    y_hat = b0 + b1*x1 + b2*x2 + b3*x1*x2 + b4*(x1)**2 + b5*(x2)**2 + ...    # <-- Some of the terms # the expression can get complicated\n",
    "                        # Numpy's polyfit function cannot perform this type of regression\n",
    "                        # Use the 'preprocessing' library in scikit-learn\n",
    "                        \n",
    "                        # Hwa be3mel new data frame feha new features Ex: \n",
    "                        # lw 3andy two features fel original data frame (x1, x2) w 3ayz a3mal 3rd order two dimentional polynomial regression \n",
    "                        # El new data frame hykoon feha five features x1, x2, x1*x2, (x1)**2, (x2)**2\n",
    "                        # B3d keda we will use linear regression to find the parameters b0, b1, b2, b3, b4, b5 as if el features di kanet totally independent from the beginning <-- ta2reeban di 3'alat because the dimentionality of the model is still two 'the number of the original features'  --> MESH FAHEM 7AGA!\n",
    "                        # **T2reeban lw ana fahem s7: hwa by3ml keda bas to find the parameters b0, b1, b2, b3, b4, b5.. b3dan we use the equation in original dimention 3ady\n",
    "                    \n",
    "                            from sklearn.preprocessing import PolynomialFeatures\n",
    "                            pr = PolynomialFeatures(degree, include_bias=False)\n",
    "                            pr.fit_transform(df[['predictor_1', 'predictor_2']])\n",
    "                    \n",
    "                # Steps: Normalization --> Polynomial Transform --> Liner Regression\n",
    "                \n",
    "                # Preprocessing\n",
    "                    from sklearn.preprocessing import StandardScaler\n",
    "                    scale = StandardScaler()\n",
    "                    scale.fit(x_df)\n",
    "                    scale.transform(x_df)\n",
    "            \n",
    "            # Pipelines are a way to simplify the code\n",
    "            from sklearn.pipeline import Pipeline\n",
    "            Input = [('scale', StandardScaler()), ('polynomial', PolynomialFeatures(degree)), ('model', LinearRegression())]\n",
    "            pipe = Pipeline(Input)\n",
    "            pipe.fit(x, y)\n",
    "            pipe.predict(x)\n",
    "            \n",
    "            # Polly Plot function\n",
    "            def PollyPlot(xtrain, xtest, y_train, y_test, lr,poly_transform):\n",
    "                width = 12\n",
    "                height = 10\n",
    "                plt.figure(figsize=(width, height))\n",
    "\n",
    "                #training data \n",
    "                #testing data \n",
    "                # lr:  linear regression object \n",
    "                #poly_transform:  polynomial transformation object \n",
    "\n",
    "                xmax=max([xtrain.values.max(), xtest.values.max()])\n",
    "                xmin=min([xtrain.values.min(), xtest.values.min()])\n",
    "                x=np.arange(xmin, xmax, 0.1)\n",
    "\n",
    "                plt.plot(xtrain, y_train, 'ro', label='Training Data')\n",
    "                plt.plot(xtest, y_test, 'go', label='Test Data')\n",
    "                plt.plot(x, lr.predict(poly_transform.fit_transform(x.reshape(-1, 1))), label='Predicted Function')\n",
    "                plt.ylim([-10000, 60000])\n",
    "                plt.ylabel('target')\n",
    "                plt.legend()\n",
    "                    \n",
    "        # Measures for In-Sample Evaluation \n",
    "            # Using the trainging data to evaluate the model (does not give an estimate of how well the model can predict new data) so we will split the data in the upcoming section\n",
    "            # These measures are a way to numerically determine how good the model fits on our data\n",
    "                # Mean Squared Error - MSE\n",
    "                    Σ (y - y_hat)**2 / Number of Samples\n",
    "                \n",
    "                    from sklearn.metrics import mean_squared_error\n",
    "                    mean_squared_error(y, y_hat)\n",
    "                    \n",
    "                # R-squared - R^2\n",
    "                    # The coefficient of determination\n",
    "                    # Is a measure to determine how close the data is to the fitted regression line\n",
    "                    # It could be thought of as comparing a regression model to a simple model (the mean of the data points) the model should perform much better than just the mean\n",
    "                    R^2 = (1 - MSE of the regression line / MSE of the average of the data)\n",
    "                    # It takes values between 0 and 1 (for the most part) 1 --> Good fit\n",
    "                    # If the R^2 value is negative, it can be due to overfitting\n",
    "                    lm.score(X, Y)    # R^2\n",
    "                    \n",
    "                    lm.score(x_test, y_test)      # Score for test set\n",
    "                    lm.score(x_train, y_train)    # Score for train set\n",
    "                    \n",
    "                    # R^2 tells us what percent of the variability in the dependent variable is accounted for by the regression on the independent variable\n",
    "                    # An R^2 of 1 means that all movement of another dependent variable are completely explained by movment in te independent variables\n",
    "                    # An acceptable value for R^2 depends on what field you are studing and what you use case is\n",
    "\n",
    "                # Polynomial Fit (score)\n",
    "                    from sklearn.metrics import r2_score\n",
    "                    r_squared = r2_score(y, p(x))\n",
    "                    mean_squared_error(df['target'], p(x))\n",
    "                    \n",
    "        # Prediction and Decision Making\n",
    "            # Do the the predicted values make sense\n",
    "            # Use visualizations to plot results\n",
    "            # Numerical measures for evaluation\n",
    "            # Comparing models\n",
    "            # MLR, SLR\n",
    "            \n",
    "            # You can use np.arrange to create fictional test data for the predictor variable to check the performence of the model (predicting the target variable)\n",
    "            new_input = np.arrange(start, end+1, step).reshape(-1,1)\n",
    "            new_y_hat = lm.predict(new_input)    \n",
    "            \n",
    "            # Use scatter plot to visualize and residual plot and distribution plot\n",
    "            # Mean Squared Error - MSE: the most intuitive numerical measure for determining if a model is good or not\n",
    "            # As the sqaure error incresses, the targets get further from predictied points\n",
    "            # A lower mean square error does not necessarily imply better fit\n",
    "            # MSE for an MLR model will be smaller than the MSE for an SLR model, since the errors of the data will decrease when more variables are included in the model\n",
    "            # Polynomial regression will also have a smaller MSE than regular regression\n",
    "            \n",
    "        # Interpret the R-square and the Mean Square Error: Interpret R-square (x 100) as the percentage of the variation in the response variable y that is explained by the variation in explanatory variable(s) x.\n",
    "        # The Mean Squared Error tells you how close a regression line is to a set of points. It does this by taking the average distances from the actual points to the predicted points and squaring them.\n",
    "        \n",
    "        # Decision Making: Determining a Good Model Fit\n",
    "            # What is a good R-squared value?\n",
    "                # When comparing models, the model with the higher R-squared value is a better fit for the data.\n",
    "\n",
    "            # What is a good MSE?\n",
    "                # When comparing models, the model with the smallest MSE value is a better fit for the data.\n",
    "        \n",
    "        # In many cases MLR is better than SLR and polynomial fit\n",
    "        \n",
    "        # Search online: polynomial fit - polynomial regression - the difference between them and MLR and SLR\n",
    "        ---------------\n",
    "\n",
    "        # Model Evaluation and Refinement\n",
    "\n",
    "            # Model Evaluation and Refinement\n",
    "                from sklearn.model_selection import train_test_split\n",
    "                    # Out of sample data --> approximate how the model performs in the real word\n",
    "                    # When we finish testing the model we sould use all the data to train the model\n",
    "                x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=0)\n",
    "                \n",
    "                # Generalization error is a measure of how well our data does at predicting previously unseen data\n",
    "                # The error we obtain using our testing data is an approximation of this erroe\n",
    "                \n",
    "                # Using more data for training and less for testing, gives us an accurate means of determining how well our model will perform in the real world but the precesion of the performance will e low\n",
    "                    # Training the model with different combination of training and testing samples from same dataset, the results are close to the generalization error but distinct form each other\n",
    "                \n",
    "                # Using more data for testing and less for training, the accuraxy of the generalization performance will be less but the model will have good precesion\n",
    "                    # All our error estimates are relatively close together, but there are further away from the true generalization error\n",
    "                \n",
    "                # To overcome this problem we use:\n",
    "                # Cross Validation\n",
    "                    # One of the most common out of sample evaluation metrics\n",
    "                    # The dataset is split into k equal groups (folds) some for training and some for testing\n",
    "                    # If the data is split into 4 folds, 1 is used for training and the rest (3) for training\n",
    "                    # Then we use another fold for testing and the rest for training until each fold is used for both training and testing\n",
    "                    # At the end, we use the average results as the estimate of out-of-sample erroe, the evaluation metric depends on the model\n",
    "                \n",
    "                from sklearn.model_selection import cross_val_score    # R^2\n",
    "                    scores = cross_val_score(lr, x_data, y_data, cv=k)    # The results are the scores for each time a different fold (proportion) was used for testing\n",
    "                    np.mean(scores)    # We can calculate the average and standard deviation of our estimate 'cross.std'\n",
    "                    # We can use negative squared error as a score by setting the parameter 'scoring' metric to 'neg_mean_squared_error'\n",
    "                    -1 * cross_val_score(lr, x_data, y_data, cv=k, scoring='neg_mean_squared_error')\n",
    "                \n",
    "                # To fined the values predicted by the model (used in calculating the score)\n",
    "                from sklearn.model_selection import cross_val_predict\n",
    "                    y_hat = cross_val_predict(lr, x_data, y_data, cv=k)    # Each time the function uses a fold for testing and the rest for training, it produces an output (predicted values) then calculates the score\n",
    "            \n",
    "            # Overfitting, Underfitting and Model Selection\n",
    "                # We assume the training data comes from a polynomial function plus some noise \n",
    "                    y(x) + noise\n",
    "                # The goal of model selection is te determine the order of the polynomial toprovide the best estimate of the function\n",
    "                # Underfitting --> the model is to simple to fit the data\n",
    "                # Overfitting  --> the model is too flexible and fits the noise rather than the function\n",
    "                \n",
    "                # The training error decreases with the order of the polynomial\n",
    "                # The test error is a better means of estimating the error of the polynomial, it decreases till the best error then increases again\n",
    "                # We select the order that minimize the test error\n",
    "                # We can use R^2 to choose the right order (use a for loop to test different orders with the model)\n",
    "                \n",
    "            # Ridge Regression    (prevents overfitting)\n",
    "                # The estimated coefficients (b0, b1, b2, ...) have a very larg magnitude especially the higher order polynomials\n",
    "                # Ridge regression controls these coefficient using the parameter alpha, as alpha increases the coefficiecnts get smaller especially the higher order polynomials features\n",
    "                # Carefull! if alpha is too large the coefficient will approach zero and underfit the data\n",
    "                # Ridge (alpha) reduces the effect of outliers on the model\n",
    "                # Ridge regression is a regression that is employed in a Multiple regression model when Multicollinearity occurs\n",
    "                \n",
    "                # In order to select alpha we use cross validation\n",
    "                # Validation data is a set of data similar to test data but it is used to select parameters (like alpha)\n",
    "                # we try different values for alpha and train the model then calculate R^2 or (MSE) then choose the best model\n",
    "                # The overfitting problem is worse if we have lots of features\n",
    "                # Plot different values of alpha on x axis and R^2 on y axis when using the validation data to test the model and the training data to test the model then plot two different lines on the same graph (validation and training)\n",
    "                \n",
    "                from sklearn.linear_model import Ridge\n",
    "                ridge = Ridge(alpha)\n",
    "                ridge.fit(x, y)\n",
    "                y_hat = ridge.predict(x)\n",
    "\n",
    "            # Grid Search\n",
    "                # Allows to scan through multiple free parameters (hyperparameters like alpha and k)\n",
    "                # We can automatically iterate over hyper parameters using crossvalidation (called Grid Search)\n",
    "                # Grid search takes the model you want to train and different values of hyperparameters \n",
    "                # Then calculate MSE or R^2 for various hyperparameter values, allowing to choose the best values\n",
    "                # Grid search is a time-efficient tuning technique that exhaustively computes the optimum values of hyperparameters performed on specific parameter values of estimators.\n",
    "                # To select the hyperparameter we split our dataset into three parts: the training set, validation set, test set \n",
    "                    # We train the model for different hyperparameters\n",
    "                    # We select the hyperparameters that minimize the MSE and maximize the R^2 on the validation set\n",
    "                    # We test our model performance using the test set\n",
    "            \n",
    "                from sklearn.linear_model import Ridge\n",
    "                from sklearn.model_selection import GridSearchCV\n",
    "                parameters = [{'parameter_1' : [parameter_1_value_1, parameter_1_value_2,...], 'parameter_2' : [parameter_2_value_1, parameter_2_value_2,...], ...}]\n",
    "                RR = Ridge()    # Or any model\n",
    "                grid = GridSearchCV(RR, parameters, cv=k, iid=None)    # In order to avoid a deprecation warning due to the iid parameter, we set the value of iid to \"None\"\n",
    "                grid.fit(x_data, y_data)\n",
    "                BestRR = grid.best_estimator_\n",
    "                scores = grid.cv_results_    # Mean score on the validation data\n",
    "                scores['mean_test_score']\n",
    "                BestRR.score(x_test, y_test)\n",
    "                \n",
    "                # Ridge regression has the option to normalize the data, We can put 'normalize' as parameter_2 with values True, False\n",
    "                \n",
    "                # We can print out the score for the different free parameter values\n",
    "                for param, mean_val, mean_test in zip(scores['params'], scores['mean_test_score'], scores['mean_train_score'])\n",
    "                    print(param, 'R^2 on test data', mean_val, 'R^2 on train data', mean_test)\n",
    "                    \n",
    "                df=df._get_numeric_data() # If you want to only use numeric data (columns)\n",
    "                \n",
    "                # Ex Linear Regression:\n",
    "                pr = PolynomialFeatures(degree=5)         # Use a for loop to try diffrent degrees (orders) and compare R^2 (score on test set) \n",
    "                x_train_pr = pr.fit_transform(x_train)    # Usally he uses only one predictor x_train['predictor'], can we do the same thing but using multiple predictors --> yes you can\n",
    "                x_test_pr = pr.fit_transform(x_test)    \n",
    "                poly = LinearRegression()\n",
    "                poly.fit(x_train_pr, y_train)\n",
    "                yhat = poly.predict(x_test_pr)\n",
    "                poly.score(x_train_pr, y_train)\n",
    "                poly.score(x_test_pr, y_test)             # Use this one\n",
    "                \n",
    "                # Ex Ridge\n",
    "                pr=PolynomialFeatures(degree=2)\n",
    "                x_train_pr=pr.fit_transform(x_train)\n",
    "                x_test_pr=pr.fit_transform(x_test)\n",
    "                RigeModel=Ridge(alpha=1)\n",
    "                RigeModel.fit(x_train_pr, y_train)\n",
    "                yhat = RigeModel.predict(x_test_pr)\n",
    "                \n",
    "                # We select the value of alpha that minimizes the test error. To do so, we can use a for loop. We have also created a progress bar to see how many iterations we have completed so far.\n",
    "                from tqdm import tqdm\n",
    "                Rsqu_test = []\n",
    "                Rsqu_train = []\n",
    "                dummy1 = []\n",
    "                Alpha = 10 * np.array(range(0,1000))\n",
    "                pbar = tqdm(Alpha)\n",
    "\n",
    "                for alpha in pbar:\n",
    "                    RigeModel = Ridge(alpha=alpha) \n",
    "                    RigeModel.fit(x_train_pr, y_train)\n",
    "                    test_score, train_score = RigeModel.score(x_test_pr, y_test), RigeModel.score(x_train_pr, y_train)\n",
    "\n",
    "                    pbar.set_postfix({\"Test Score\": test_score, \"Train Score\": train_score})\n",
    "\n",
    "                    Rsqu_test.append(test_score)\n",
    "                    Rsqu_train.append(train_score)\n",
    "                \n",
    "                # We can plot out the value of R^2 for different alphas\n",
    "                width = 12\n",
    "                height = 10\n",
    "                plt.figure(figsize=(width, height))\n",
    "\n",
    "                plt.plot(Alpha,Rsqu_test, label='validation data  ')\n",
    "                plt.plot(Alpha,Rsqu_train, 'r', label='training Data ')\n",
    "                plt.xlabel('alpha')\n",
    "                plt.ylabel('R^2')\n",
    "                plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
